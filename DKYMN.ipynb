{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DKYMN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r-PX6ao2BYs"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "class AssismentData():\n",
        "    def __init__(self):\n",
        "        self.data = pd.read_csv(\"/content/drive/My Drive/DKT/2015_100_skill_builders_main_problems.csv\")\n",
        "\n",
        "        self.data.dropna()\n",
        "\n",
        "        self.data[\"user_id\"], _ = pd.factorize(self.data[\"user_id\"])\n",
        "        self.data[\"sequence_id\"], _ = pd.factorize(self.data[\"sequence_id\"])\n",
        "        self.data[\"skills\"] = self.data.apply(lambda x: x.sequence_id * 2 if x.correct == 0.0 else x.sequence_id * 2 + 1, axis=1)\n",
        "\n",
        "        self.data = self.data.drop(columns=\"log_id\", axis=1)\n",
        "\n",
        "        self.data = self.data.groupby(\"user_id\").filter(lambda q: len(q) > 1).copy()\n",
        "\n",
        "        self.seq = self.data.groupby('user_id').apply(\n",
        "            lambda r: (\n",
        "                r[\"sequence_id\"].values,\n",
        "                r['skills'].values,\n",
        "                r['correct'].values\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.train = self.seq.sample(frac=0.8)\n",
        "        self.test = self.seq[~self.seq.index.isin(self.train.index)]\n",
        "\n",
        "\n",
        "    def datasetReturn(self, data, shuffle=None, batch_size=50, val_data=None):\n",
        "        dataset = tf.data.Dataset.from_generator(lambda: data, output_types=(tf.int32, tf.int32, tf.int32))\n",
        "\n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=shuffle)\n",
        "\n",
        "        MASK_VALUE = -1\n",
        "        dataset = dataset.padded_batch(\n",
        "            batch_size=batch_size,\n",
        "            padding_values=(MASK_VALUE, MASK_VALUE, MASK_VALUE),\n",
        "            padded_shapes=([None], [None], [None]),\n",
        "            drop_remainder=True\n",
        "        )\n",
        "\n",
        "        return dataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PtXsNbBGxbm"
      },
      "source": [
        "ass = AssismentData()\n",
        "train_data,test_data = ass.datasetReturn(ass.train),ass.datasetReturn(ass.test)\n",
        "val_log = 'log/val'\n",
        "train_loss_log = 'log/train'\n",
        "summary_writer = tf.summary.create_file_writer(val_log)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6sM4qFdGymF"
      },
      "source": [
        "\n",
        "total_skills_correctness = 200\n",
        "total_skills = 100\n",
        "embedding_size = 100\n",
        "batchsize = 50\n",
        "M = 50\n",
        "class DKVMNcell(tf.keras.layers.AbstractRNNCell):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        self.units = units\n",
        "        super(DKVMNcell, self).__init__(**kwargs)\n",
        "        self.Mv = self.add_weight(shape=(M, embedding_size),\n",
        "                                  initializer='random_normal',\n",
        "                                  trainable=True)\n",
        "        self.Mv = tf.expand_dims(self.Mv,axis=0)\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return self.units\n",
        "\n",
        "    def call(self, w_attention, erase_signal_mul, add_signal_mul, states):\n",
        "        \"\"\"\n",
        "        :param w_attention: 这个应该是concept矩阵计算后的注意力权重\n",
        "        :param erase_signal: erase标志\n",
        "        :param add_signal: add标志\n",
        "        :param states: Mk矩阵\n",
        "        :return: r，Mv\n",
        "        \"\"\"\n",
        "        # 读\n",
        "        # w_attention.shape (50,50) state.shape (50,50,100)\n",
        "        r = tf.matmul(tf.expand_dims(w_attention,axis=1),states)\n",
        "        \n",
        "        # print(r.shape)(50,1,100)\n",
        "        r = r[:,0,:]\n",
        "        \n",
        "        # 写\n",
        "        states = states * erase_signal_mul + add_signal_mul\n",
        "       \n",
        "        return r, states\n",
        "\n",
        "\n",
        "class DKVMN(tf.keras.models.Model):\n",
        "    def __init__(self):\n",
        "        super(DKVMN, self).__init__()\n",
        "        # 掩码层\n",
        "        self.mask = tf.keras.layers.Masking(mask_value=-1)\n",
        "        # 题目嵌入\n",
        "        self.exercise_embedding = tf.keras.layers.Embedding(total_skills, embedding_size)\n",
        "        # 题目对错嵌入\n",
        "        self.exercise_correctness_embedding = tf.keras.layers.Embedding(total_skills_correctness, embedding_size)\n",
        "\n",
        "        self.cell = DKVMNcell(10)\n",
        "\n",
        "        self.Mk = self.add_weight(shape=(M, embedding_size),\n",
        "                                  initializer='random_normal',\n",
        "                                  trainable=True)\n",
        "\n",
        "        self.erase = tf.keras.layers.Dense(embedding_size)\n",
        "        self.add = tf.keras.layers.Dense(embedding_size, activation=\"tanh\")\n",
        "        self.r = tf.keras.layers.Dense(embedding_size, activation=\"tanh\")\n",
        "        self.p = tf.keras.layers.Dense(2,activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, skillid, skill_correctness,correctness):\n",
        "        shape = skillid.shape\n",
        "        skill_correctness = tf.expand_dims(skill_correctness, axis=-1)\n",
        "        skillid = tf.expand_dims(skillid, axis=-1)\n",
        "        # 掩码\n",
        "        skillid = self.mask(skillid)\n",
        "        skill_correctness = self.mask(skill_correctness)\n",
        "        # 映射\n",
        "        skill_embedding = self.exercise_embedding(skillid)\n",
        "        skill_correctness_embedding = self.exercise_correctness_embedding(skill_correctness) #（batch,seqlen,embeddingsize）\n",
        "        skill_correctness_embedding = tf.squeeze(skill_correctness_embedding, axis=2) \n",
        "        skill_embedding = tf.squeeze(skill_embedding, axis=2)\n",
        "        # tensorlist_batch = tf.TensorArray(dtype=tf.float32,size=0,dynamic_size=True)\n",
        "        # for k in range(skill_correctness_embedding.shape[0]):\n",
        "        #   tensorlist = tf.TensorArray(tf.float32,size=0,dynamic_size=True)\n",
        "        #   for i in range(skill_correctness_embedding.shape[1]):\n",
        "        #     # 做对拼接1 ，做错拼接0\n",
        "        #     w=tensorlist.write(i,tf.cond(correctness[k,i],lambda:tf.zeros_like(skill_correctness_embedding[k,i]),lambda:tf.ones_like(skill_correctness_embedding[k,i])))\n",
        "        #     w.mark_used()\n",
        "        #   w=tensorlist_batch.write(k,tensorlist.stack())\n",
        "        #   w.mark_used()\n",
        "        # # 拼接后skill_correctness_embedding\n",
        "        # skill_correctness_embedding = tf.concat([skill_correctness_embedding,tensorlist_batch.stack()],axis=-1)\n",
        "        \n",
        "        # 产生 注意力权重\n",
        "        w_attention = tf.matmul(skill_embedding, tf.expand_dims(tf.transpose(self.Mk), axis=0))\n",
        "        w_attention = tf.nn.softmax(w_attention)\n",
        "\n",
        "        #  遗忘 和 更新 Mv的过程\n",
        "        erase_signal = self.erase(skill_correctness_embedding)\n",
        "        add_signal = self.add(skill_correctness_embedding)\n",
        "\n",
        "        erase_signal_mul = 1 - tf.expand_dims(w_attention, axis=-1) * tf.expand_dims(erase_signal, axis=2)\n",
        "        add_signal_mul = tf.expand_dims(w_attention, axis=-1) * tf.expand_dims(add_signal, axis=2)\n",
        "        # 遗忘和更新Mv\n",
        "        # batch个 Mv\n",
        "        states = self.cell.Mv\n",
        "        for i in range(batchsize)[1:]:\n",
        "          states = tf.concat([states,self.cell.Mv],axis=0)\n",
        "    \n",
        "        cell_out_list = tf.TensorArray(size=0,dynamic_size=True,dtype=tf.float32)\n",
        "\n",
        "        for i in range(shape[1]):\n",
        "            r,states = self.cell(w_attention[:,i],erase_signal_mul[:,i],add_signal_mul[:,i],states)\n",
        "            w = cell_out_list.write(i,tf.expand_dims(r,axis=1))\n",
        "            w.mark_used()\n",
        "        f = cell_out_list.read(0)\n",
        "        for i in range(shape[1])[1:]:\n",
        "            f = tf.concat([f,cell_out_list.read(i)],axis=1)\n",
        "        \n",
        "        r = tf.concat([f,skill_embedding], axis=-1)\n",
        "        loss = tf.nn.softmax(self.p(self.r(r)))\n",
        "        return loss\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iMISTOLKFmL"
      },
      "source": [
        "\n",
        "def test_one_step(skillid,skill_correctness,correctness):\n",
        "    probility = dkvmn(skillid, skill_correctness,correctness)\n",
        "\n",
        "    mask = 1 - tf.cast(tf.equal(correctness, -1), tf.int32)\n",
        "\n",
        "    mask = tf.squeeze(mask)\n",
        "    # mask掉\n",
        "    probility = tf.boolean_mask(probility, mask)\n",
        "    label = tf.boolean_mask(correctness, mask)\n",
        "\n",
        "    label = tf.one_hot(label, depth=2)\n",
        "   \n",
        "    vauc.update_state(label,probility)\n",
        "\n",
        "def train_one_step(skillid,skill_correctness,correctness):\n",
        "    with tf.GradientTape() as tape:\n",
        "        probility = dkvmn(skillid,skill_correctness,correctness)\n",
        "        mask = 1 - tf.cast(tf.equal(correctness,-1),tf.int32)\n",
        "        mask = tf.squeeze(mask)\n",
        "        # mask 掉\n",
        "        probility = tf.boolean_mask(probility,mask)\n",
        "        label = tf.boolean_mask(correctness,mask)\n",
        "        \n",
        "        label = tf.one_hot(label, depth=2)\n",
        "        # 求bc\n",
        "        bc.update_state(label,probility)\n",
        "\n",
        "        loss = tf.losses.categorical_crossentropy(label,probility)\n",
        "       \n",
        "        auc.update_state(label,probility)\n",
        "        \n",
        "        gradients = tape.gradient(loss, dkvmn.trainable_variables)\n",
        "        # 反向传播，自动微分计算\n",
        "        optimizer.apply_gradients(zip(gradients, dkvmn.trainable_variables))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llj2Gnc_Joel"
      },
      "source": [
        "dkvmn = DKVMN()\n",
        "bc = tf.metrics.CategoricalCrossentropy()\n",
        "auc = tf.metrics.AUC()\n",
        "vauc = tf.metrics.AUC()\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.01)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNJGTP9XJpvX"
      },
      "source": [
        "import time\n",
        "for epoch in range(10):\n",
        "    start = time.time()\n",
        "    train_data = train_data.shuffle(32)\n",
        "    auc.reset_states()\n",
        "    vauc.reset_states()\n",
        "    bc.reset_states()\n",
        "    for  s, v, l in train_data.as_numpy_iterator():\n",
        "        train_one_step(s, v, l)\n",
        "\n",
        "    for s, v, l in test_data.as_numpy_iterator():\n",
        "        test_one_step(s, v, l)\n",
        "    print(time.time()-start)\n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar('train_auc', auc.result(), step=epoch)\n",
        "        tf.summary.scalar('val_auc', vauc.result(), step=epoch)\n",
        "\n",
        "    print(bc.result(), auc.result(), vauc.result())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOmWNlI__wb3",
        "outputId": "2922a80f-c48a-4f88-e585-c74fd0025917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vauc.reset_states()\n",
        "for s, v, l in test_data.as_numpy_iterator():\n",
        "    test_one_step(s, v, l)\n",
        "print(vauc.result())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.7753244, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}